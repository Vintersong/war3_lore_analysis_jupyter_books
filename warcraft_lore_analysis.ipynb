{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14a599b",
   "metadata": {},
   "source": [
    "# Warcraft Lore Analysis\n",
    "\n",
    "This notebook analyzes Warcraft lore through web scraping and text analysis to identify thematic patterns in character narratives. We'll examine themes like duty, power, sacrifice, corruption, and freedom across major characters like Arthas, Illidan, and Thrall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b17bea",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Notebook Overview\n",
    "\n",
    "**Purpose:** Computational analysis of philosophical archetypes (Stoic, Cynic, Platonic Tyrant) in Warcraft transmedia narratives.\n",
    "\n",
    "**What This Does:**\n",
    "- Scrapes Warcraft lore from Wowpedia and (optionally) Wowhead quest databases\n",
    "- Extracts passages containing keywords: duty, power, sacrifice, corruption, freedom\n",
    "- Analyzes keyword distribution across Arthas, Illidan, and Thrall\n",
    "- Exports data to CSV for qualitative coding in your paper analysis\n",
    "- Generates visualizations and clusters thematic passages\n",
    "\n",
    "**âœ… To Run:** Simply execute \"Run All\" (Ctrl+Shift+F10) and the notebook will complete all steps automatically.\n",
    "\n",
    "**â±ï¸ Estimated Time:** 5-10 minutes for Wowpedia scraping, 30+ minutes if Selenium/Wowhead is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bce819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP: Imports, Configuration, and Function Definitions ===\n",
    "print(\"=\" * 60)\n",
    "print(\"WARCRAFT LORE ANALYSIS - Philosophical Archetype Extraction\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n[1/4] Loading libraries...\")\n",
    "\n",
    "# 1. Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import PyPDF2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ“ Core libraries loaded\")\n",
    "\n",
    "# 2. Configuration\n",
    "KEYWORDS = ['duty', 'power', 'sacrifice', 'corruption', 'freedom']\n",
    "URLS = {\n",
    "    'arthas': ['https://wowpedia.fandom.com/wiki/Arthas_Menethil'],\n",
    "    'illidan': ['https://wowpedia.fandom.com/wiki/Illidan_Stormrage'],\n",
    "    'thrall': ['https://wowpedia.fandom.com/wiki/Thrall']\n",
    "}\n",
    "NOVELS = ['arthas_rise.pdf', 'illidan.pdf', 'lord_clans.pdf']\n",
    "USE_PDFS = False\n",
    "\n",
    "# Optional: Enable Wowhead scraping (slow, may not work due to JavaScript/HTML changes)\n",
    "# Set to True only if you need comprehensive quest data and have 30+ minutes\n",
    "USE_WOWHEAD = False\n",
    "\n",
    "print(f\"âœ“ Configuration loaded (Keywords: {', '.join(KEYWORDS)})\")\n",
    "print(f\"  Characters: Arthas, Illidan, Thrall\")\n",
    "print(f\"  PDF novels: {'Enabled' if USE_PDFS else 'Disabled'}\")\n",
    "print(f\"  Wowhead quests: {'Enabled' if USE_WOWHEAD else 'Disabled (use Wowpedia only)'}\")\n",
    "\n",
    "# 3. Define core scraping functions\n",
    "print(\"\\n[2/4] Defining analysis functions...\")\n",
    "\n",
    "def scrape_page(url, char):\n",
    "    \"\"\"Scrape Wowpedia page for keyword-containing passages.\"\"\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    texts = [p.text for p in soup.find_all('p') if any(kw.lower() in p.text.lower() for kw in KEYWORDS)]\n",
    "    return [{'char': char, 'source': url, 'text': t, 'keywords': [kw for kw in KEYWORDS if kw.lower() in t.lower()]} for t in texts]\n",
    "\n",
    "def extract_novel(pdf_path, char):\n",
    "    \"\"\"Extract keyword passages from PDF novels.\"\"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        texts = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if any(kw.lower() in text.lower() for kw in KEYWORDS):\n",
    "                texts.append({\n",
    "                    'char': char, \n",
    "                    'source': pdf_path, \n",
    "                    'text': text[:500], \n",
    "                    'keywords': [kw for kw in KEYWORDS if kw.lower() in text.lower()]\n",
    "                })\n",
    "    return texts\n",
    "\n",
    "def extract_context(text, keyword, window=50):\n",
    "    \"\"\"Extract context around keyword occurrences (Â±50 chars).\"\"\"\n",
    "    kw_pos = [m.start() for m in re.finditer(re.escape(keyword), text, re.IGNORECASE)]\n",
    "    contexts = []\n",
    "    for pos in kw_pos:\n",
    "        start = max(0, pos - window)\n",
    "        end = min(len(text), pos + window + len(keyword))\n",
    "        contexts.append(text[start:end])\n",
    "    return contexts\n",
    "\n",
    "print(\"âœ“ Wowpedia scraper defined\")\n",
    "print(\"âœ“ PDF parser defined\")\n",
    "print(\"âœ“ Context extraction defined\")\n",
    "\n",
    "# 4. Check Selenium availability (optional for Wowhead)\n",
    "print(\"\\n[3/4] Checking optional dependencies...\")\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    \n",
    "    def scrape_wowhead_with_selenium(chars, expansions=['wotlk']):\n",
    "        \"\"\"Scrape Wowhead quests using Selenium (handles JavaScript rendering).\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--log-level=3')  # Suppress logs\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        data = []\n",
    "        \n",
    "        try:\n",
    "            for exp in expansions:\n",
    "                for char in chars:\n",
    "                    url = f'https://www.wowhead.com/{exp}/quests?filter=cr=160;cs=1;str={char.lower()}'\n",
    "                    print(f\"  Scraping {exp}/{char}...\")\n",
    "                    \n",
    "                    driver.get(url)\n",
    "                    time.sleep(5)\n",
    "                    \n",
    "                    quest_links = driver.find_elements(By.CSS_SELECTOR, 'a.q, a.q2, a.q1')\n",
    "                    \n",
    "                    for link in quest_links[:20]:\n",
    "                        try:\n",
    "                            title = link.text\n",
    "                            quest_url = link.get_attribute('href')\n",
    "                            \n",
    "                            if not quest_url or 'quest=' not in quest_url:\n",
    "                                continue\n",
    "                            \n",
    "                            driver.get(quest_url)\n",
    "                            time.sleep(2)\n",
    "                            \n",
    "                            text = \"\"\n",
    "                            for selector in ['.text', '.objective', '.q-description']:\n",
    "                                try:\n",
    "                                    elem = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                                    text += elem.text + \" \"\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            if text and any(kw.lower() in text.lower() for kw in KEYWORDS):\n",
    "                                data.append({\n",
    "                                    'char': char,\n",
    "                                    'expansion': exp,\n",
    "                                    'source': quest_url,\n",
    "                                    'title': title,\n",
    "                                    'text': text[:1500],\n",
    "                                    'keywords': [kw for kw in KEYWORDS if kw.lower() in text.lower()]\n",
    "                                })\n",
    "                            \n",
    "                            time.sleep(1)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    SELENIUM_AVAILABLE = True\n",
    "    print(\"âœ“ Selenium available (Wowhead scraping enabled)\")\n",
    "except ImportError:\n",
    "    SELENIUM_AVAILABLE = False\n",
    "    print(\"âš  Selenium not installed (Wowhead scraping disabled)\")\n",
    "    print(\"  Install with: pip install selenium webdriver-manager\")\n",
    "\n",
    "print(\"\\n[4/4] Setup complete! Starting data collection...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f2380",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "This cell loads all libraries, defines scraping functions, and checks for optional dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf460b6",
   "metadata": {},
   "source": [
    "## Data Collection: Wowpedia\n",
    "\n",
    "Scrapes character pages for passages containing the five philosophical keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845fdb4",
   "metadata": {},
   "source": [
    "## 5. Data Collection and Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA COLLECTION: Scrape Wowpedia ===\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: WOWPEDIA SCRAPING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Scrape Wowpedia character pages\n",
    "print(\"\\nScraping Wowpedia for thematic passages...\")\n",
    "for char, urls in URLS.items():\n",
    "    for url in urls:\n",
    "        if url.startswith('http'):\n",
    "            try:\n",
    "                scraped = scrape_page(url, char)\n",
    "                data.extend(scraped)\n",
    "                print(f\"  âœ“ {char.capitalize()}: {len(scraped)} passages from Wowpedia\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error scraping {char}: {e}\")\n",
    "\n",
    "# Parse PDF novels (if enabled)\n",
    "if USE_PDFS:\n",
    "    print(\"\\nParsing PDF novels...\")\n",
    "    novel_mappings = [\n",
    "        ('arthas_rise.pdf', 'arthas'),\n",
    "        ('illidan.pdf', 'illidan'), \n",
    "        ('lord_clans.pdf', 'thrall')\n",
    "    ]\n",
    "    \n",
    "    for novel, char in novel_mappings:\n",
    "        try:\n",
    "            novel_data = extract_novel(novel, char)\n",
    "            data.extend(novel_data)\n",
    "            print(f\"  âœ“ {char.capitalize()}: {len(novel_data)} passages from {novel}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  âš  {novel} not found (skipping)\")\n",
    "else:\n",
    "    print(\"\\n  PDF parsing disabled (set USE_PDFS=True to enable)\")\n",
    "\n",
    "# Extract context snippets around keywords\n",
    "print(\"\\nExtracting keyword contexts...\")\n",
    "for passage in data:\n",
    "    context_passages = []\n",
    "    for kw in KEYWORDS:\n",
    "        context_passages.extend(extract_context(passage['text'], kw))\n",
    "    passage['context'] = ' | '.join(set(context_passages))\n",
    "\n",
    "print(f\"\\nâœ“ Wowpedia collection complete: {len(data)} total passages\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA COLLECTION: Scrape Wowhead (Optional) ===\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: WOWHEAD QUEST SCRAPING (OPTIONAL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if USE_WOWHEAD and SELENIUM_AVAILABLE:\n",
    "    print(\"\\nSelenium enabled - scraping Wowhead quest database...\")\n",
    "    print(\"â±ï¸ This may take 10-30 minutes (JavaScript rendering + rate limiting)\\n\")\n",
    "    \n",
    "    try:\n",
    "        selenium_quests = scrape_wowhead_with_selenium(['Thrall', 'Arthas'], expansions=['wotlk'])\n",
    "        \n",
    "        if len(selenium_quests) > 0:\n",
    "            selenium_quests['count'] = selenium_quests['keywords'].apply(len)\n",
    "            selenium_quests.to_csv('wowhead_quests_selenium.csv', index=False)\n",
    "            \n",
    "            print(f\"\\nâœ“ Wowhead collection complete: {len(selenium_quests)} quests\")\n",
    "            print(\"\\nKeyword distribution by character/expansion:\")\n",
    "            print(selenium_quests.groupby(['char', 'expansion'])['count'].sum())\n",
    "        else:\n",
    "            print(\"\\nâš  No quests collected from Wowhead\")\n",
    "            print(\"  Possible causes: JavaScript timeout, changed HTML structure, or no keyword matches\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Wowhead scraping failed: {e}\")\n",
    "        print(\"  Continuing with Wowpedia data only...\")\n",
    "else:\n",
    "    if not USE_WOWHEAD:\n",
    "        print(\"\\nâ­ï¸ Wowhead scraping disabled (USE_WOWHEAD=False)\")\n",
    "        print(\"  Set USE_WOWHEAD=True in the Setup cell to enable quest scraping\")\n",
    "    elif not SELENIUM_AVAILABLE:\n",
    "        print(\"\\nâš  Selenium not installed - Wowhead scraping unavailable\")\n",
    "        print(\"  Install with: pip install selenium webdriver-manager\")\n",
    "    print(\"  Continuing with Wowpedia data only...\")\n",
    "    print(\"  To enable: pip install selenium webdriver-manager\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c360741",
   "metadata": {},
   "source": [
    "## Data Collection: Wowhead (Optional)\n",
    "\n",
    "Uses Selenium to scrape quest text from Wowhead's JavaScript-rendered pages. Skipped if Selenium unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354cae3a",
   "metadata": {},
   "source": [
    "## Keyword Analysis & Visualization\n",
    "\n",
    "Analyzes keyword frequency across characters and generates visualizations for your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ANALYSIS: Keyword Distribution & Visualization ===\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: KEYWORD ANALYSIS & VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nAnalyzing {len(df)} passages...\")\n",
    "    \n",
    "    # Calculate keyword frequency\n",
    "    df['count'] = df['keywords'].apply(len)\n",
    "    dense_passages = df[df['count'] >= 1].head(100)\n",
    "    \n",
    "    # Export for qualitative coding\n",
    "    dense_passages.to_csv('warcraft_passages.csv', index=False)\n",
    "    print(f\"âœ“ Exported {len(dense_passages)} passages â†’ warcraft_passages.csv\")\n",
    "    \n",
    "    # Generate visualization\n",
    "    print(\"\\nGenerating keyword frequency chart...\")\n",
    "    char_counts = dense_passages.groupby('char')['count'].sum()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    char_counts.plot.bar(ax=ax, color=['#C41E3A', '#0070DE', '#8B4513'])\n",
    "    ax.set_title('Keyword Frequency by Character (Duty, Power, Sacrifice, Corruption, Freedom)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Total Keyword Count', fontsize=12)\n",
    "    ax.set_xlabel('Character', fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('keyword_frequency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"KEYWORD FREQUENCY BY CHARACTER\")\n",
    "    print(\"=\" * 40)\n",
    "    for char, count in char_counts.items():\n",
    "        print(f\"  {char.capitalize():12} : {count:3} keyword occurrences\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Keyword breakdown\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"KEYWORD DISTRIBUTION\")\n",
    "    print(\"=\" * 40)\n",
    "    all_keywords = [kw for keywords in df['keywords'] for kw in keywords]\n",
    "    keyword_counts = Counter(all_keywords)\n",
    "    for kw, count in keyword_counts.most_common():\n",
    "        print(f\"  {kw.capitalize():15} : {count:3} occurrences\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâœ— No data collected - check URLs and connection\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282aa1b4",
   "metadata": {},
   "source": [
    "## Thematic Clustering\n",
    "\n",
    "Groups passages into clusters to identify philosophical patterns (Stoic duty vs. Cynic freedom vs. Tyrant power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ANALYSIS: Text Clustering (Thematic Patterns) ===\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: THEMATIC CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "if len(dense_passages) > 0:\n",
    "    print(f\"\\nClustering {len(dense_passages)} passages into 3 thematic groups...\")\n",
    "    \n",
    "    # Vectorize text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "    X = vectorizer.fit_transform(dense_passages['text'])\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    dense_passages['cluster'] = kmeans.fit_predict(X)\n",
    "    \n",
    "    print(\"âœ“ Clustering complete\\n\")\n",
    "    \n",
    "    # Display sample passages from each cluster\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAMPLE PASSAGES BY THEMATIC CLUSTER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for cluster_id in range(3):\n",
    "        print(f\"\\n--- CLUSTER {cluster_id} ---\")\n",
    "        cluster_passages = dense_passages[dense_passages['cluster'] == cluster_id]\n",
    "        cluster_size = len(cluster_passages)\n",
    "        print(f\"Size: {cluster_size} passages\")\n",
    "        \n",
    "        if cluster_size > 0:\n",
    "            # Show character distribution in cluster\n",
    "            char_dist = cluster_passages['char'].value_counts()\n",
    "            print(f\"Characters: {dict(char_dist)}\")\n",
    "            \n",
    "            # Show top keywords in cluster\n",
    "            cluster_keywords = [kw for keywords in cluster_passages['keywords'] for kw in keywords]\n",
    "            top_keywords = Counter(cluster_keywords).most_common(3)\n",
    "            print(f\"Top keywords: {', '.join([kw for kw, _ in top_keywords])}\")\n",
    "            \n",
    "            # Show sample passage\n",
    "            sample = cluster_passages.iloc[0]\n",
    "            print(f\"\\nSample passage ({sample['char']}):\")\n",
    "            print(f\"  {sample['text'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Export clustered data\n",
    "    dense_passages.to_csv('warcraft_passages_clustered.csv', index=False)\n",
    "    print(\"\\nâœ“ Exported clustered data â†’ warcraft_passages_clustered.csv\")\n",
    "    \n",
    "    # Cluster distribution\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    print(dense_passages.groupby(['cluster', 'char']).size().unstack(fill_value=0))\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâš  No passages available for clustering\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ“Š Output files generated:\")\n",
    "print(\"  â€¢ warcraft_passages.csv - All extracted passages\")\n",
    "print(\"  â€¢ warcraft_passages_clustered.csv - With cluster assignments\")\n",
    "print(\"  â€¢ keyword_frequency.png - Visualization chart\")\n",
    "if SELENIUM_AVAILABLE:\n",
    "    print(\"  â€¢ wowhead_quests_selenium.csv - Quest data (if Selenium ran)\")\n",
    "print(\"\\nâœ… Ready for qualitative coding in your philosophical archetype analysis!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
